{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_class_visualization(target_y, model, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate an image to maximize the score of target_y under a pretrained model.\n",
    "    \n",
    "    Inputs:\n",
    "    - target_y: Integer in the range [0, 1000) giving the index of the class\n",
    "    - model: A pretrained CNN that will be used to generate the image\n",
    "    \n",
    "    Keyword arguments:\n",
    "    - l2_reg: Strength of L2 regularization on the image\n",
    "    - learning_rate: How big of a step to take\n",
    "    - num_iterations: How many iterations to use\n",
    "    - blur_every: How often to blur the image as an implicit regularizer\n",
    "    - max_jitter: How much to gjitter the image as an implicit regularizer\n",
    "    - show_every: How often to show the intermediate result\n",
    "    \"\"\"\n",
    "    l2_reg = kwargs.pop('l2_reg', 1e-3)\n",
    "    learning_rate = kwargs.pop('learning_rate', 25)\n",
    "    num_iterations = kwargs.pop('num_iterations', 100)\n",
    "    blur_every = kwargs.pop('blur_every', 10)\n",
    "    max_jitter = kwargs.pop('max_jitter', 16)\n",
    "    show_every = kwargs.pop('show_every', 25)\n",
    "\n",
    "    X = 255 * np.random.rand(224, 224, 3)\n",
    "    X = preprocess_image(X)[None]\n",
    "    \n",
    "    ########################################################################\n",
    "    # TODO: Compute the loss and the gradient of the loss with respect to  #\n",
    "    # the input image, model.image. We compute these outside the loop so   #\n",
    "    # that we don't have to recompute the gradient graph at each iteration #\n",
    "    #                                                                      #\n",
    "    # Note: loss and grad should be TensorFlow Tensors, not numpy arrays!  #\n",
    "    #                                                                      #\n",
    "    # The loss is the score for the target label, target_y. You should     #\n",
    "    # use model.classifier to get the scores, and tf.gradients to compute  #\n",
    "    # gradients. Don't forget the (subtracted) L2 regularization term!     #\n",
    "    ########################################################################\n",
    "    loss = model.classifier[0, target_y] # scalar loss\n",
    "    grad = tf.gradients(loss, model.image) # gradient of loss with respect to model.image, same size as model.image\n",
    "    grad = tf.squeeze(grad) - l2_reg*2*model.image\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Randomly jitter the image a bit; this gives slightly nicer results\n",
    "        ox, oy = np.random.randint(-max_jitter, max_jitter+1, 2)\n",
    "        Xi = X.copy()\n",
    "        X = np.roll(np.roll(X, ox, 1), oy, 2)\n",
    "        \n",
    "        ########################################################################\n",
    "        # TODO: Use sess to compute the value of the gradient of the score for #\n",
    "        # class target_y with respect to the pixels of the image, and make a   #\n",
    "        # gradient step on the image using the learning rate. You should use   #\n",
    "        # the grad variable you defined above.                                 #\n",
    "        #                                                                      #\n",
    "        # Be very careful about the signs of elements in your code.            #\n",
    "        ########################################################################\n",
    "        gradient_step = sess.run(grad, feed_dict={model.image:X})\n",
    "        X += learning_rate * gradient_step\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        # Undo the jitter\n",
    "        X = np.roll(np.roll(X, -ox, 1), -oy, 2)\n",
    "\n",
    "        # As a regularizer, clip and periodically blur\n",
    "        X = np.clip(X, -SQUEEZENET_MEAN/SQUEEZENET_STD, (1.0 - SQUEEZENET_MEAN)/SQUEEZENET_STD)\n",
    "        if t % blur_every == 0:\n",
    "            X = blur_image(X, sigma=0.5)\n",
    "\n",
    "        # Periodically show the image\n",
    "        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n",
    "            plt.imshow(deprocess_image(X[0]))\n",
    "            class_name = class_names[target_y]\n",
    "            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n",
    "            plt.gcf().set_size_inches(4, 4)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
